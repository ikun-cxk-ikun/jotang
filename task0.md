# task0
### 1.机器学习与深度学习 
**机器学习**：人工智能的内核
**深度学习**： **机器学习**的一种方式
**区别**:人工智能>机器学习>深度学习 （**子集**）
### 2. 监督学习与无监督学习
**监督学习**：需要带有标签的数据来调整模型的参数。“标签”就是数据的正确答案或分类，比如图像分类中的“猫”或“狗”，或是房价或股票预测中的具体数值。监督学习的目标是学习输入数据到标签之间的映射关系。
 **无监督学习**：处理的数据没有显式的标签或已知的结果变量。从数据中发现隐藏的模式或结构，比如将数据自动划分为不同的群组，这些群组内的数据点具有相似性，而不同群组或簇的数据点则相异。
 
 
### 3.偏导数、链式法则、梯度、矩阵等数学概念在机器学习中的作用
**偏导数**：表示函数在某个特定变量方向上的变化率。在机器学习中，偏导数主要用于计算损失函数的梯度。通过计算损失函数对各个参数的偏导数，确定损失函数在特定点上的下降或上升最快的方向，根据偏导数的方向更新模型参数，以最小化损失函数。
**链式法则**：用于计算复合函数的导数，在机器学习中，链式法则用于计算损失函数对模型参数的梯度。当损失函数由多个简单函数复合而成时，链式法则通过计算每个简单函数的导数，并将它们相乘来得到复合函数的导数。
**梯度**：表示函数在某个点上的方向导数在该点处的最大值，即函数在该点上升或下降最快的方向。在机器学习中，梯度用于指导模型参数的更新方向。通过计算损失函数的梯度，调整参数来最快地降低损失函数值。
**矩阵**：矩阵可以用来表示和处理高维数据，如图像、文本和音频等。这些数据可以通过矩阵运算进行转换、降维和特征提取等操作。模型参数的估计、预测结果的计算等需要通过矩阵运算来实现。


### 4.激活函数
**Sigmoid 函数**：
- **定义**：将任意实值压缩到 (0, 1) 区间内，表达式为 σ(x)=1/1+e**（-x）
输出范围在 (0, 1) 之间，适合用于二分类问题的输出层。Sigmoid 函数的导数=σ（1-σ），导数图像是关于0对称的钟型图像，最大值是0.25
- **缺点**：容易出现梯度消失问题，因为当输入值远离 0 时，函数的梯度趋于 0；且其输出不是以 0 为中心，可能会影响梯度下降的效率。

**Tanh 函数（双曲正切函数）**：
- **定义**：Tanh 函数将任意实值压缩到 (-1, 1) 区间内，表达式为**tanh(x)= (exp(x) - exp(-x))/(exp(x) + exp(-x))**
- **优点**：相比 Sigmoid 函数，Tanh 函数的输出以 0 为中心，这有助于加速梯度下降的收敛速度。
- **缺点**：仍然存在梯度消失的问题，特别是在输入值很大或很小时。

**ReLU 函数（修正线性单元）**:
- **定义**：ReLU 函数表达式为 **f(x)=max(0,x)**
- **优点**：计算简单且高效，不存在梯度消失问题（在正区间内）；收敛速度通常比 Sigmoid 和 Tanh 快。
- **缺点**：存在Dead ReLU ，即当输入为负时，神经元将不再被激活，导致梯度无法传播；同时，ReLU 的输出不是以 0 为中心，Relu 函数无上界，如果线性单元输出过大或网络是循环结构，会出现梯度爆炸。

**Leaky ReLU 函数**：
- **定义**：Leaky ReLU 函数是在 ReLU 函数的负半轴增加一个很小的梯度值（如0.01）
- **优点**：保留了 ReLU 的优点，同时避免了 Dead ReLU 问题。
  
**PReLU 函数**：
- 定义：PReLU 函数将 ReLU 函数中的负半轴改为ax，其中 a 是一个可学习的参数。
- 优点：相比于 Leaky ReLU，PReLU 通过学习参数 a 来更好地适应不同的数据集和网络结构。

**ELU 函数**：
- **定义**：ELU 函数结合了 ReLU 和 Sigmoid 的特性，将负半轴改为a（e**x-1）
- **优点**：缓解了梯度消失问题，并且由于负值部分的存在，使得输出的均值接近 0，有助于加快收敛速度

**Softmax 函数**：
- **定义**：Softmax 函数通常用于多分类问题的输出层，它将输入值映射为概率分布。
- **优点**：能够将输出转换为概率分布，方便进行多分类任务。

**Swish 函数**：
- **定义**：Swish 函数是一种自门控激活函数，其表达式为 f(x)=x⋅σ(βx)，其中 σ 是 Sigmoid 函数，β 是一个常数或可训练的参数。
- **优点**：结合了线性函数和 ReLU 函数的优点，具有无上界有下界、平滑、非单调的特性。
  


### 5.神经网络的基本结构
神经网络的基本结构通常包括以下几个主要组成部分:

1. **输入层**:
- 接收原始数据输入
- 每个输入特征对应一个神经元

2. **隐藏层**:
- 位于输入层和输出层之间
- 可以有一层或多层
- 对输入进行非线性变换
- 提取和学习特征

3. **输出层**:
- 产生网络的最终输出
- 神经元数量取决于任务类型(如分类、回归等)

4. **神经元**:
- 网络的基本计算单元
- 接收输入,计算加权和,应用激活函数,产生输出

5. **连接权重**:
- 连接不同层神经元
- 表示输入特征的重要性
- 在训练过程中不断调整

6. **激活函数**:
- 引入非线性
- 常见的有ReLU、Sigmoid、Tanh等

7. **偏置**:
- 为每个神经元增加一个可学习的常数项
- 提高模型的灵活性
- 偏置允许神经元在输入为零时也有非零输出

8.**损失函数**：
- 损失函数用于量化网络预测与实际结果之间的差异。
- 在训练过程中，目标是最小化损失函数的值。

9.**优化算法**：
- 优化算法用于更新神经网络的权重和偏置，以最小化损失函数的值。
- 常见的优化算法包括梯度下降、随机梯度下降等。

### 6.损失函数
**均方函数**：
均方误差（Mean Squared Error, MSE）是衡量模型预测值与真实值之间差异的一种常用方法。它是预测误差平方的平均值，反映了模型预测值偏离真实值的程度。均方误差越小，说明模型的预测能力越强，预测值越接近真实值。

均方误差的计算公式为：

$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

其中：
- $n$ 是样本数量。
- $y_i$ 是第 $i$ 个样本的真实值。
- $\hat{y}_i$ 是第 $i$ 个样本的预测值。
- $\sum_{i=1}^{n} (y_i - \hat{y}_i)^2$ 表示所有样本预测误差的平方和。

均方误差被广泛应用于回归分析、机器学习等领域中。然而，均方误差对异常值（即那些远离真实值的数据点）比较敏感，因为异常值会导致误差的平方值急剧增大，从而影响整体的均方误差值。

**交叉熵损失**:
- 适用于分类任务，主要用于度量两个概率分布间的差异性信息。
- 公式（二分类）：\[ \text{CE} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)] \]
- 公式（多分类）：\[ \text{CE} = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{k} y_{ij} \log(\hat{y}_{ij}) \]
- 其中 \( y_i \) 是真实标签，\( \hat{y}_i \) 是预测概率。

### 7.学习率：
**如果梯度下降过程中，每次都按照相同的步幅收敛，则可能错过极值点，所以每次在之前的不服减小一定比率，这个比率称之为“学习率”**

### 8.卷积运算：
在卷积运算中，特别是卷积神经网络（CNN）的卷积层中，输出矩阵（或称为特征图）的大小是一个关键的计算参数。这个大小取决于输入矩阵的大小、卷积核（滤波器）的大小、步长（stride）以及填充（padding）的大小。以下是一个通用的计算公式和相关的解释：

#### 输出矩阵大小计算公式

对于二维卷积（常用于图像处理），输出矩阵的大小（假设为 $O_h \times O_w$，其中 $O_h$ 是高度，$O_w$ 是宽度）可以通过以下公式计算：

\[
O_h = \left\lfloor \frac{I_h - F_h + 2P_h}{S_h} \right\rfloor + 1
\]

\[
O_w = \left\lfloor \frac{I_w - F_w + 2P_w}{S_w} \right\rfloor + 1
\]

其中：
- $I_h$ 和 $I_w$ 分别是输入矩阵的高度和宽度。
- $F_h$ 和 $F_w$ 分别是卷积核的高度和宽度。
- $P_h$ 和 $P_w$ 分别是输入矩阵在高度和宽度方向上的填充大小。
- $S_h$ 和 $S_w$ 分别是卷积核在高度和宽度方向上的步长。
- $\left\lfloor \cdot \right\rfloor$ 表示向下取整操作，确保输出矩阵的尺寸是整数。

#### 参数解释

- **卷积核（Filter）**：用于与输入矩阵的局部区域进行卷积运算的矩阵，也称为滤波器。
- **步长（Stride）**：卷积核在输入矩阵上滑动时每次移动的像素数。步长为1时，卷积核会逐个像素地滑动；步长大于1时，卷积核会跳过一些像素。
- **填充（Padding）**：在输入矩阵的边界周围添加额外的零值像素（或其他值），以控制输出矩阵的大小。填充分为“valid”和“same”两种模式：“valid”模式不进行填充（即 $P_h = P_w = 0$），“same”模式通过填充来确保输出矩阵与输入矩阵具有相同的尺寸（但可能需要通过计算来确定具体的填充值）。

#### 示例

假设有一个 $6 \times 6$ 的输入矩阵，使用 $3 \times 3$ 的卷积核，步长为1，不进行填充（即 $P_h = P_w = 0$）。根据上面的公式，输出矩阵的大小为：

\[
O_h = \left\lfloor \frac{6 - 3 + 0}{1} \right\rfloor + 1 = 4
\]

\[
O_w = \left\lfloor \frac{6 - 3 + 0}{1} \right\rfloor + 1 = 4
\]

因此，输出矩阵的大小为 $4 \times 4$。
