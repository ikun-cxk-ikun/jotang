# task0
#### 1.机器学习与深度学习 
**机器学习**：人工智能的内核
**深度学习**： **机器学习**的一种方式
**区别**:人工智能>机器学习>深度学习 （**子集**）
#### 2. 监督学习与无监督学习
**监督学习**：需要带有标签的数据来调整模型的参数。“标签”就是数据的正确答案或分类，比如图像分类中的“猫”或“狗”，或是房价或股票预测中的具体数值。监督学习的目标是学习输入数据到标签之间的映射关系。
 **无监督学习**：处理的数据没有显式的标签或已知的结果变量。从数据中发现隐藏的模式或结构，比如将数据自动划分为不同的群组，这些群组内的数据点具有相似性，而不同群组或簇的数据点则相异。
 
 
 #### 3.偏导数、链式法则、梯度、矩阵等数学概念在机器学习中的作用
**偏导数**：表示函数在某个特定变量方向上的变化率。在机器学习中，偏导数主要用于计算损失函数的梯度。通过计算损失函数对各个参数的偏导数，确定损失函数在特定点上的下降或上升最快的方向，根据偏导数的方向更新模型参数，以最小化损失函数。
**链式法则**：用于计算复合函数的导数，在机器学习中，链式法则用于计算损失函数对模型参数的梯度。当损失函数由多个简单函数复合而成时，链式法则通过计算每个简单函数的导数，并将它们相乘来得到复合函数的导数。
**梯度**：表示函数在某个点上的方向导数在该点处的最大值，即函数在该点上升或下降最快的方向。在机器学习中，梯度用于指导模型参数的更新方向。通过计算损失函数的梯度，调整参数来最快地降低损失函数值。
**矩阵**：矩阵可以用来表示和处理高维数据，如图像、文本和音频等。这些数据可以通过矩阵运算进行转换、降维和特征提取等操作。模型参数的估计、预测结果的计算等需要通过矩阵运算来实现。


#### 4.激活函数
**Sigmoid 函数**将任意实值压缩到 **(0, 1)** 区间内，表达式为 σ(x)=1/1+e**（-x）
输出范围在 (0, 1) 之间，适合用于二分类问题的输出层。
Sigmoid 函数的导数=σ（1-σ），导数图像是关于0对称的钟型图像，最大值是0.25
容易出现梯度消失问题，因为当输入值远离 0 时，函数的梯度趋于 0；且其输出不是以 0 为中心，可能会影响梯度下降的效率。
**Tanh 函数（双曲正切函数）**：Tanh 函数将任意实值压缩到 **(-1, 1)** 区间内，表达式为**tanh(x)= (exp(x) - exp(-x))/(exp(x) + exp(-x))**
相比 Sigmoid 函数，Tanh 函数的输出以 0 为中心，这有助于加速梯度下降的收敛速度。
仍然存在梯度消失的问题，特别是在输入值很大或很小时。
**ReLU 函数（）**:ReLU 函数表达式为 **f(x)=max(0,x)**
计算简单且高效，不存在梯度消失问题（在正区间内）；收敛速度通常比 Sigmoid 和 Tanh 快。
 Dead ReLU ，即当输入为负时，神经元将不再被激活，导致梯度无法传播；同时，ReLU 的输出不是以 0 为中心
****
****