# task0
#### 1.机器学习与深度学习 
**机器学习**：人工智能的内核
**深度学习**： **机器学习**的一种方式
**区别**:人工智能>机器学习>深度学习 （**子集**）
#### 2. 监督学习与无监督学习
**监督学习**：需要带有标签的数据来调整模型的参数。“标签”就是数据的正确答案或分类，比如图像分类中的“猫”或“狗”，或是房价或股票预测中的具体数值。监督学习的目标是学习输入数据到标签之间的映射关系。
 **无监督学习**：处理的数据没有显式的标签或已知的结果变量。从数据中发现隐藏的模式或结构，比如将数据自动划分为不同的群组，这些群组内的数据点具有相似性，而不同群组或簇的数据点则相异。
 
 
 #### 3.偏导数、链式法则、梯度、矩阵等数学概念在机器学习中的作用
**偏导数**：表示函数在某个特定变量方向上的变化率。在机器学习中，偏导数主要用于计算损失函数的梯度。通过计算损失函数对各个参数的偏导数，确定损失函数在特定点上的下降或上升最快的方向，根据偏导数的方向更新模型参数，以最小化损失函数。
**链式法则**：用于计算复合函数的导数，在机器学习中，链式法则用于计算损失函数对模型参数的梯度。当损失函数由多个简单函数复合而成时，链式法则通过计算每个简单函数的导数，并将它们相乘来得到复合函数的导数。
**梯度**：表示函数在某个点上的方向导数在该点处的最大值，即函数在该点上升或下降最快的方向。在机器学习中，梯度用于指导模型参数的更新方向。通过计算损失函数的梯度，调整参数来最快地降低损失函数值。
**矩阵**：矩阵可以用来表示和处理高维数据，如图像、文本和音频等。这些数据可以通过矩阵运算进行转换、降维和特征提取等操作。模型参数的估计、预测结果的计算等需要通过矩阵运算来实现。


#### 4.激活函数
**Sigmoid 函数**：
- **定义**：将任意实值压缩到 (0, 1) 区间内，表达式为 σ(x)=1/1+e**（-x）
输出范围在 (0, 1) 之间，适合用于二分类问题的输出层。Sigmoid 函数的导数=σ（1-σ），导数图像是关于0对称的钟型图像，最大值是0.25
- **缺点**：容易出现梯度消失问题，因为当输入值远离 0 时，函数的梯度趋于 0；且其输出不是以 0 为中心，可能会影响梯度下降的效率。

**Tanh 函数（双曲正切函数）**：
- **定义**：Tanh 函数将任意实值压缩到 (-1, 1) 区间内，表达式为**tanh(x)= (exp(x) - exp(-x))/(exp(x) + exp(-x))**
- **优点**：相比 Sigmoid 函数，Tanh 函数的输出以 0 为中心，这有助于加速梯度下降的收敛速度。
- **缺点**：仍然存在梯度消失的问题，特别是在输入值很大或很小时。

**ReLU 函数（修正线性单元）**:
- **定义**：ReLU 函数表达式为 **f(x)=max(0,x)**
- **优点**：计算简单且高效，不存在梯度消失问题（在正区间内）；收敛速度通常比 Sigmoid 和 Tanh 快。
- **缺点**：存在Dead ReLU ，即当输入为负时，神经元将不再被激活，导致梯度无法传播；同时，ReLU 的输出不是以 0 为中心，Relu 函数无上界，如果线性单元输出过大或网络是循环结构，会出现梯度爆炸。

**Leaky ReLU 函数**：
- **定义**：Leaky ReLU 函数是在 ReLU 函数的负半轴增加一个很小的梯度值（如0.01）
- **优点**：保留了 ReLU 的优点，同时避免了 Dead ReLU 问题。
  
**PReLU 函数**：
- 定义：PReLU 函数将 ReLU 函数中的负半轴改为ax，其中 a 是一个可学习的参数。
- 优点：相比于 Leaky ReLU，PReLU 通过学习参数 a 来更好地适应不同的数据集和网络结构。

**ELU 函数**：
- **定义**：ELU 函数结合了 ReLU 和 Sigmoid 的特性，将负半轴改为a（e**x-1）
- **优点**：缓解了梯度消失问题，并且由于负值部分的存在，使得输出的均值接近 0，有助于加快收敛速度

**Softmax 函数**：
- **定义**：Softmax 函数通常用于多分类问题的输出层，它将输入值映射为概率分布。
- **优点**：能够将输出转换为概率分布，方便进行多分类任务。

**Swish 函数**：
- **定义**：Swish 函数是一种自门控激活函数，其表达式为 f(x)=x⋅σ(βx)，其中 σ 是 Sigmoid 函数，β 是一个常数或可训练的参数。
- **优点**：结合了线性函数和 ReLU 函数的优点，具有无上界有下界、平滑、非单调的特性。
  


#### 5.神经网络的基本结构
