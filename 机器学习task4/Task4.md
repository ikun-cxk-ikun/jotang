# Task4
#### 低熵
* 高熵意味着低信息增益，表示系统的不确定性高，信息含量大。
* 低熵则意味着高信息增益，表示系统的不确定性低，信息含量小。
 **例1**：假设有一个数据集，其中包含红色和蓝色两种类别的样本。如果数据集中红色样本和蓝色样本的数量相等，那么此时数据集的熵最高，因为不确定性最大。而如果数据集中只有一种颜色的样本（例如全是红色样本），那么此时数据集的熵最低，因为不确定性为零。
 **例2**：比如说输入提示语“第一任美国总统是”，输出结果是固定的“华盛顿”，这种序列就是低熵序列，那么，输入提示语“美国总统是”，输出的结果就有多种可能，把这种序列叫做高熵序列。
**主要问题**：通常，低熵序列由于其具有及其确定的特性，人工输出的答案和机器生成的答案将会达到高度吻合，这给水印的加注造成困难，因为水印的加注就是为了区分人和机器生成的文字，从差异量判断一段文字是人书写的还是机器生成的。
#### 语言模型
语言模型的核心是一个词汇表，词汇表中包含被“标记”的各个单词片段，我们称之为令牌（token），典型的词汇表包含50000个以上的单词片段。在语言模型实际运行过程中，前端会输入一段提示语（promote），经过词汇表的处理后我们将提示语根据单词切割成各个片段，并且针对每一个片段给与一个标记，这些令牌按照一定顺序组成的集合（tokens）就是我们解析得到的计算机可识别的提示语序列。

#### Transformer
Transformer模型主要由两个部分组成：编码器（Encoder）和解码器（Decoder）。

1. **编码器**：编码器由多个相同的层堆叠而成，每个层都包含两个子层，即多头自注意力（Multi-Head Self-Attention）和位置全连接前馈网络（Position-wise Feed-Forward Network）。这两个子层都使用了残差连接和层归一化。编码器的主要任务是将输入序列转换为信息的向量表示。
2. **解码器**：解码器同样由多个相同的层堆叠而成，但每个层包含三个子层，即多头自注意力、多头编码器-解码器注意力（Multi-Head Encoder-Decoder Attention）和位置全连接前馈网络。解码器的主要任务是根据编码器的输出和之前已经生成的输出序列，逐步生成目标序列的每个元素。

#### 两种水印算法

1. **Hard Watermark（硬水印）**

   - **原理**：
     Hard Watermark是一种较为直接的水印嵌入方法。它首先将词汇表随机切分为“红色”和“绿色”两部分。在模型生成文本时，算法会强制模型只从“绿色”token集合中选择，而忽略“红色”。这样做的好处是方法简单，但缺点也很明显，即会严重影响生成文本的质量，特别是在一些低熵（low-entropy）场景下。

   - **实现**：
     - 根据生成序列预测下一个词的概率向量 p^t。
     - 根据生成序列最后一个单词确定 random seed。
     - 根据 random seed 确定 green list 以及 red list 的切分。
     - 只从 green list 中挑选 token，忽略 red list 中的 token。
     - 这样的做法的好处是简单，但严重影响了生成文本的质量，比如一些低熵情形。如 Barack 后面在绝大部分情况下的预测是 Obama，但如果此时 Obama 在 red list 中，模型会强行选用 green list 中的另一个词。这显然不行。

   - **思考**：
     硬水印方法虽然简单直接，但在实际应用中会遇到很多问题。例如，在一些高度确定的文本生成场景下（如“第一任美国总统是”），硬水印可能会导致模型输出与预期不符的词汇，从而影响文本质量。
 ### **以解决这个问题，作者提出了更有效的平替**
 
1. **Soft Watermark（软水印）**

   - **原理**：
     Soft Watermark同样将词汇表切分为“红色”和“绿色”两部分，但在模型输出时，不是强制选择“绿色”token，而是对属于“绿色”token的tokens增加常数δ，提高它们在logits向量中的权重，从而整体增大“绿色”token的预测概率。此方法相比于 Hard Watermarking 更加有灵活度，不会显著降低输出文本质量，同时保证低熵情形下模型的正确输出 （即使 Obama 是 red token，输出单词 Barack 之后仍然下一个输出它）。

   - **实现**：
     具体实现时，针对每次模型输出的logits向量，对属于green list的tokens增加常数δ。这样相当于提高了green list tokens的logits权重，从而整体增大了green tokens的预测概率。（公式没看懂思密达）

   - **思考**：
     软水印方法相较于硬水印更加灵活和实用。它能够在保证文本质量的同时嵌入水印，且不易被察觉。特别是在低熵场景下，软水印方法依然能够保持模型的正确输出。
#### 实验
没看太懂，只能写个大概（╥﹏╥）
作者从 C4 新闻数据集随机选取了若干文本，作者将每段文本从尾部切分成定长序列，作为 baseline，剩下的部分则作为 prompt。作者从数据集中多项采样 （multinomial sampling，其实就是随机采），直到获得至少 500 条长度 200 (+-5) token 的生成序列

当 green token 比例 γ 很小且 green bias δ 很大时，即便超短文本也可以嵌入很强的水印。然而过强的水印也会扰乱生成文本

（a）（b）（c）三张图展示了不同超参组合下水印强度 (平均 z-score) 和文本质量之间的关系，二者负相关作者发现小 green list ( γ = 0.1) 时整体达到 Pareto 最优。

作者在附录部分提供了一些 emperical 的例子，直观展示 watermarking 对于文本质量的影响。Table 3 展示的是高熵例子，而 Table 4 展示的是低熵例子。

#### Beam Search 提升水印文本质量？
作者认为 Beam Search 和水印方法有协同作用，特别是当 8-way Beam Search 时，水印强度对文本质量几乎无影响 (原文：”showing very little perplexity cost to achieve strong watermarking”)。从图中可以看出当 8-way Beam Search 应用时，对应绿色点连线基本与 x 轴垂直。
从 a, b 图来看，在不同超参组合下，水印强度 z-score 随 token 数量上升（样本基数越大，水印越显著），同时更小的 green list 以及更大的 green bias 也导致 z-score 更大。

c 图展示 8-way Beam Search 下，水印强度得到普遍增强，当 δ  仅为 2 时，只需要 T 大于 25 就可以使得 z-score 平均值大于 5。(因此Beam Search 不仅保证文本效果，水印也更明显了)。
#### 水印攻击方式及思考

- **人为替换**：
  攻击者可以手动替换文本中的一些“绿色”token为“红色”token，从而破坏水印的完整性。

- **利用语言模型paraphrase**：
  攻击者可以利用其他语言模型对文本进行改写（paraphrase），从而改变原文中的“绿色”token分布。这种方法更加隐蔽和复杂，因为改写后的文本在语义上可能与原文保持一致，但水印已经被破坏。

- **进行精心的替换**：
  攻击者可以通过一些小但精心的改变，比如在文本中插入多余的空格或者错误拼写一些词。当然一个比较成熟的水印检测器能够先正则化(normalize)文本，使得它能够忽略这样多余的空格，因此这种表层的修改不是一个大问题。

- **修改tokenization**：
  攻击者可以通过修改文本的tokenization方式，从而改变“绿色”token的分布。例如，通过改变词汇的拆分方式或添加额外的标点符号等方式来破坏水印。比如将 “life.\nVerrilius” 修改为 “life.Verrilius”，去除了中间的换行符。这样就将原先的 BPE 分词 V_err_ili_us 变为 Ver_r_ili_us，这将导致更多的 red tokens，降低水印强度。
**思考**：这些攻击方式之所以有效，主要是因为它们能够在一定程度上改变文本中的“绿色”token分布，从而降低水印的检测准确性。